{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Homework 4: Big Data"}, {"metadata": {}, "cell_type": "markdown", "source": "This homework assignment builds on the in-class work we did with Spark.\nYou will be using the [Yelp Academic Dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset) and focusing primaily on the text of the reviews (i.e. the reviews.json.gz file).\n\n**We suggest that you work in groups to make a plan to tackle this homework assignment.**"}, {"metadata": {}, "cell_type": "markdown", "source": "Here are the two questions that comprise the assignment:\n\n1. List the 50 most common non-stopword words that are unique to *positive* reviews.\n2. List the 50 most common non-stopword words that are unique to *negative* reviews."}, {"metadata": {}, "cell_type": "markdown", "source": "As an example, consider the following two reviews:\n\n* Positive: The meal was great, and the service was the best we ever experienced.\n* Negative: The meal was awful.  It was the worst thing we ever experienced.\n\nAssume our stopwords are {'the','was','and','the','was','we','it'}\n\n* Positive unique: {'great', 'service', 'best'}\n\n* Negative unique: {'awful', 'worst', 'thing'}\n\nIn this example, each unique word occurs just once, so the concept of \"top 50\" doesn't make sense.  For your data, you'll need to count the number of times each unique word occurs."}, {"metadata": {}, "cell_type": "markdown", "source": "Because this is the final homework assignment in this course, we are leaving it up to you to operationalize most of the details.  For example, you will need to determine what constitutes a positive or a negative review.\n\n**You should take care to document your work, preferably using markdown blocks. In-code commenting is also \na good idea.**\n\nYou will also need to generate a list of stopwords.  Neither spaCy nor NLTK are available on AWS EMR, so you'll need to be creative in how you get a good list of stopwords into Spark.\n\nFinally, you will notice that there are a **lot** of reviews.  You might want to work off a small sample (i.e. use the rdd.sample() function in Spark) to work on a reduced size dataset while you're developing your solution."}, {"metadata": {}, "cell_type": "markdown", "source": "### REMEMBER TO TERMINATE YOUR AWS CLUSTER(S) WHEN YOU'RE DONE (OR WHEN YOU TAKE A BREAK)!"}, {"metadata": {}, "cell_type": "markdown", "source": "Please download your work in HTML and IPYNB formats and submit both to Canvas."}, {"metadata": {}, "cell_type": "markdown", "source": "This section has all of the import statmenets I need plus some additional"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "from pyspark.sql import SQLContext\nfrom pyspark import *\nfrom pyspark.sql import Row\nimport csv\nfrom pyspark.ml.feature import StopWordsRemover\nimport re\nimport pyspark.sql.functions as f\nfrom pyspark.sql.functions import col, split, udf, concat, lit", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "891c9d0e4cd14dfb9a4efda4638fc987"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1555962736889_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-26-70.ec2.internal:20888/proxy/application_1555962736889_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-28-154.ec2.internal:8042/node/containerlogs/container_1555962736889_0001_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## Importing the spark dataframe \ndf = spark.read.json('s3://umsi-data-science/data/yelp/review.json.gz')", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "775ffd5fb42d460586b72e7da6364c04"}}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## printed schema of data for future reference\ndf.printSchema()", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f4d84e692bd342f6b2a85a102e557bf2"}}, "metadata": {}}, {"output_type": "stream", "text": "root\n |-- business_id: string (nullable = true)\n |-- cool: long (nullable = true)\n |-- date: string (nullable = true)\n |-- funny: long (nullable = true)\n |-- review_id: string (nullable = true)\n |-- stars: long (nullable = true)\n |-- text: string (nullable = true)\n |-- useful: long (nullable = true)\n |-- user_id: string (nullable = true)", "name": "stdout"}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## Took a sample to make sure my method works before using on whole dataset \nsample = df.sample(False,0.0005,81)\n", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e54b7aa1b5e848658dd410145078e1b2"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Question 1: List the 50 most common non-stopword words that are unique to *positive* reviews."}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## Got a count of the sample to make sure it was a reasonable enough size \nsample.count()", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "070b27d16d984299871b9c87dca7fca6"}}, "metadata": {}}, {"output_type": "stream", "text": "2634", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "There are 2634 rows in the sample data. "}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## Take the sample and cast the text column to array<string> to work with the stopword remover in pyspark \nsamp = sample.withColumn(\"text\", split(col(\"text\"), \" \").cast(\"array<string>\"))\n\n## Removes the stopwords in the column\nremover = StopWordsRemover(inputCol=\"text\", outputCol=\"text2\")\ntemp2 = remover.transform(samp)\n\n", "execution_count": 22, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f1311d5d38d64a3ea4325812c2a3c0ba"}}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## Creates a new column called \"text2\" that has the once again concatinated data after it was separated for the array\ntemp3 = temp2.withColumn(\"text2\", f.concat_ws(\" \", \"text2\"))\n", "execution_count": 24, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "25452b534a924f3fbe8ba8e0f310d4ca"}}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## This code is getiting rid of punctuation and digits and setting the words in text2 to lowercase \ntemp4 = temp3.select(f.lower(f.regexp_replace(col(\"text2\"), \"[^0-9a-zA-Z ]\", \"\")).alias('text2'))\n", "execution_count": 34, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6b2f15f403a74eef9853eaabf0f6a998"}}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## This code i found on https://stackoverflow.com/questions/48927271/count-number-of-words-in-a-spark-dataframe\n## It sorts the words from each row of the text column after splitting them \n## and counts the number of occurances for each\n\ntemp5 = temp4.withColumn('word', f.explode(f.split(f.col('text2'), ' ')))\\\n    .groupBy('word')\\\n    .count()\\\n    .sort('count', ascending=False)", "execution_count": 67, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2a674ee03ba94ae086b0ffb63845ca7f"}}, "metadata": {}}]}, {"metadata": {"scrolled": true, "trusted": false}, "cell_type": "code", "source": "## Gets rid of the '' word that showed the most and was always the most common \ntemp6 = temp5.filter(temp5[\"word\"] != '')", "execution_count": 71, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b38b1593ddc640cbbd28775dc9d211c0"}}, "metadata": {}}, {"output_type": "stream", "text": "root\n |-- word: string (nullable = true)\n |-- count: long (nullable = false)\n\n+-----+-----+\n| word|count|\n+-----+-----+\n|place| 1338|\n| food| 1314|\n| good| 1278|\n|great| 1091|\n| like| 1004|\n+-----+-----+\nonly showing top 5 rows", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Creating Function to test everything based on successful sample above.\nHere, I realized that both lists need to be unique so I created an optional parameter I will use to ensure they are unique. "}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "def top50words(d, d2 = None):\n    samp = d.withColumn(\"text\", split(col(\"text\"), \" \").cast(\"array<string>\"))\n    remover = StopWordsRemover(inputCol=\"text\", outputCol=\"text2\")\n    temp2 = remover.transform(samp)\n    temp3 = temp2.withColumn(\"text2\", f.concat_ws(\" \", \"text2\"))\n    temp4 = temp3.select(f.lower(f.regexp_replace(col(\"text2\"), \"[^0-9a-zA-Z ]\", \"\")).alias('text2'))\n    temp5 = temp4.withColumn('word', f.explode(f.split(f.col('text2'), ' ')))\\\n    .groupBy('word')\\\n    .count()\\\n    .sort('count', ascending=False)\n    temp6 = temp5.filter(temp5[\"word\"] != '')\n    temp6 = temp6.select(col(\"word\"), col(\"count\").alias(\"occurances\"))\n    ## This part of the function makes sure lists are unique\n    if d2 != None:\n        ## Number to make DF smaller and still make sure theres 50 unique words \n        d1 = dict()\n        og_key = d2.keys()\n        for x in temp6.take(100):\n            d1[x.word] = x.occurances\n        return {k:v for k, v in d1.iteritems() if k not in og_key}\n    else: \n        d = dict()\n        for x in temp6.take(50):\n            d[x.word] = x.occurances\n        return d", "execution_count": 100, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "33afbf45380c48e8931d69d69dd45c2f"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "I'll be running the code on a sample because there's been issues with running on the whole dataset. "}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## here I'm splitting the positive reviews from negative based on how many stars they received\n## I made a positive review 5 stars and a negative 1 star so that there should be a drasitc difference in words\ndf_samp = df.sample(False,0.05,20)\np = df_samp[df_samp['stars'] == 5]\nn = df_samp[df_samp['stars'] == 1]", "execution_count": 101, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1048f3a0321e48b78d4e64dbbb5d3d46"}}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true, "trusted": false}, "cell_type": "code", "source": "## Running function on positive reviews \npos_top_50 = top50words(p)", "execution_count": 102, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5345d7c85e6946ed85e56ce8931dd214"}}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## Running functionon negative reviews\nneg_top_50 = top50words(n, pos_top_50)\n", "execution_count": 104, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5122e964bd264e7285226a34e9df880e"}}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## Shows the top 50 positive words \npos = sorted([(x[0], x[1]) for x in pos_top_50.iteritems()], key = lambda x: x[1], reverse = True)[:50]\nfor x in pos:\n    print(x)", "execution_count": 111, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "76d2386d29bf4d359d6bb1879a68e45b"}}, "metadata": {}}, {"output_type": "stream", "text": "(u'great', 62077)\n(u'place', 50696)\n(u'food', 42881)\n(u'good', 38560)\n(u'service', 34456)\n(u'time', 33214)\n(u'get', 28510)\n(u'one', 28087)\n(u'best', 28051)\n(u'like', 27597)\n(u'back', 26883)\n(u'go', 24888)\n(u'love', 24564)\n(u'really', 23327)\n(u'amazing', 23236)\n(u'always', 22898)\n(u'also', 22656)\n(u'friendly', 20584)\n(u'staff', 19602)\n(u'definitely', 17883)\n(u'well', 17856)\n(u'it', 17652)\n(u'us', 17419)\n(u'recommend', 17377)\n(u'nice', 17334)\n(u'delicious', 16855)\n(u'even', 16676)\n(u'got', 16308)\n(u'first', 14588)\n(u'try', 14328)\n(u'made', 13797)\n(u'experience', 13740)\n(u'new', 13109)\n(u'come', 13052)\n(u'everything', 12956)\n(u'make', 12921)\n(u'awesome', 12357)\n(u'went', 12238)\n(u'restaurant', 12233)\n(u'came', 12210)\n(u'every', 12012)\n(u'little', 11954)\n(u'ever', 11935)\n(u'never', 11838)\n(u'vegas', 11615)\n(u'much', 11500)\n(u'work', 11251)\n(u'fresh', 11161)\n(u'going', 11130)\n(u'day', 11065)", "name": "stdout"}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "## Shows the top 50 negative word\nneg = sorted([(x[0], x[1]) for x in neg_top_50.iteritems()], key = lambda x: x[1], reverse = True)[:50]\nfor x in neg:\n    print(x)", "execution_count": 110, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dd6d0da29a974a62904466c47f41b7f3"}}, "metadata": {}}, {"output_type": "stream", "text": "(u'said', 13489)\n(u'told', 12944)\n(u'asked', 9623)\n(u'order', 9129)\n(u'minutes', 8650)\n(u'people', 8272)\n(u'customer', 7519)\n(u'know', 7194)\n(u'called', 7178)\n(u'ordered', 7045)\n(u'another', 7016)\n(u'bad', 6418)\n(u'take', 6340)\n(u'manager', 6325)\n(u'car', 6317)\n(u'took', 6311)\n(u'give', 6157)\n(u'way', 6046)\n(u'still', 6029)\n(u'two', 5978)\n(u'2', 5977)\n(u'call', 5901)\n(u'want', 5864)\n(u'money', 5756)\n(u'left', 5391)\n(u'worst', 5302)\n(u'say', 5195)\n(u'me', 5162)\n(u'again', 5050)\n(u'room', 4979)\n(u'wait', 4901)\n(u'better', 4879)\n(u'wanted', 4868)\n(u'see', 4835)\n(u'rude', 4814)\n(u'business', 4661)\n(u'around', 4594)\n(u'i', 4579)\n(u'last', 4545)\n(u'since', 4481)\n(u'right', 4458)\n(u'3', 4456)\n(u'times', 4450)\n(u'table', 4399)\n(u'here', 4390)\n(u'horrible', 4248)\n(u'phone', 4197)\n(u'pay', 4172)\n(u'next', 4146)\n(u'nothing', 4111)", "name": "stdout"}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}